{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e22210f",
   "metadata": {},
   "source": [
    "# Implement Simple neural networks using python\n",
    "### Implementation of different activation functions used in DL models \n",
    "### Implementation of simple neural network for and/or/xor operations using basic python\n",
    "### Implement ANN model for prediction for dataset and comparative analysis of ANN model and ML model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79f2b2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ab04810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n",
      "tanh: [-0.96402758 -0.76159416  0.          0.76159416  0.96402758]\n",
      "relu: [0. 0. 0. 1. 2.]\n",
      "leaky_relu: [-0.02 -0.01  0.    1.    2.  ]\n",
      "softmax: [0.01165623 0.03168492 0.08612854 0.23412166 0.63640865]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "x_demo = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "print(\"sigmoid:\", sigmoid(x_demo))\n",
    "print(\"tanh:\", tanh(x_demo))\n",
    "print(\"relu:\", relu(x_demo))\n",
    "print(\"leaky_relu:\", leaky_relu(x_demo))\n",
    "print(\"softmax:\", softmax(x_demo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1da9d834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND predictions: [0 0 0 1]\n",
      "OR predictions: [0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "def train_perceptron(X, y, lr=0.1, epochs=20):\n",
    "    w = np.zeros(X.shape[1] + 1)\n",
    "    for _ in range(epochs):\n",
    "        for xi, yi in zip(X, y):\n",
    "            y_hat = 1 if (np.dot(w[1:], xi) + w[0]) >= 0 else 0\n",
    "            update = lr * (yi - y_hat)\n",
    "            w[1:] += update * xi\n",
    "            w[0] += update\n",
    "    return w\n",
    "\n",
    "def predict_perceptron(X, w):\n",
    "    return np.array([1 if (np.dot(w[1:], xi) + w[0]) >= 0 else 0 for xi in X])\n",
    "\n",
    "X_logic = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_and = np.array([0, 0, 0, 1])\n",
    "y_or = np.array([0, 1, 1, 1])\n",
    "\n",
    "w_and = train_perceptron(X_logic, y_and)\n",
    "w_or = train_perceptron(X_logic, y_or)\n",
    "\n",
    "print(\"AND predictions:\", predict_perceptron(X_logic, w_and))\n",
    "print(\"OR predictions:\", predict_perceptron(X_logic, w_or))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc224f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR predictions: [0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_deriv(s):\n",
    "    return s * (1 - s)\n",
    "\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "W1 = np.random.randn(2, 2)\n",
    "b1 = np.zeros((1, 2))\n",
    "W2 = np.random.randn(2, 1)\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "lr = 0.5\n",
    "for _ in range(10000):\n",
    "    z1 = np.dot(X_xor, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    error = y_xor - a2\n",
    "    d2 = error * sigmoid_deriv(a2)\n",
    "    d1 = np.dot(d2, W2.T) * sigmoid_deriv(a1)\n",
    "\n",
    "    W2 += lr * np.dot(a1.T, d2)\n",
    "    b2 += lr * np.sum(d2, axis=0, keepdims=True)\n",
    "    W1 += lr * np.dot(X_xor.T, d1)\n",
    "    b1 += lr * np.sum(d1, axis=0, keepdims=True)\n",
    "\n",
    "xor_pred = (a2 > 0.5).astype(int)\n",
    "print(\"XOR predictions:\", xor_pred.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43309d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Model  Accuracy  Precision    Recall        F1\n",
      "0         ANN (TensorFlow)  0.733766   0.622642  0.611111  0.616822\n",
      "1  ML (LogisticRegression)  0.714286   0.608696  0.518519  0.560000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "df = df.dropna()\n",
    "\n",
    "target_col = \"Outcome\" if \"Outcome\" in df.columns else df.columns[-1]\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y if y.nunique() > 1 else None\n",
    " )\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "ann = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "    tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "ann.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "ann.fit(X_train_scaled, y_train, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "ann_probs = ann.predict(X_test_scaled, verbose=0).ravel()\n",
    "ann_pred = (ann_probs >= 0.5).astype(int)\n",
    "\n",
    "ml = LogisticRegression(max_iter=1000)\n",
    "ml.fit(X_train_scaled, y_train)\n",
    "ml_pred = ml.predict(X_test_scaled)\n",
    "\n",
    "avg = \"binary\" if y.nunique() == 2 else \"weighted\"\n",
    "\n",
    "def metrics_row(name, y_true, y_pred):\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, average=avg, zero_division=0),\n",
    "        \"Recall\": recall_score(y_true, y_pred, average=avg, zero_division=0),\n",
    "        \"F1\": f1_score(y_true, y_pred, average=avg, zero_division=0),\n",
    "    }\n",
    "\n",
    "results = pd.DataFrame([\n",
    "    metrics_row(\"ANN (TensorFlow)\", y_test, ann_pred),\n",
    "    metrics_row(\"ML (LogisticRegression)\", y_test, ml_pred),\n",
    "])\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
